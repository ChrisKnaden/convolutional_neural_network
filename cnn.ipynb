{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44845f0",
   "metadata": {},
   "source": [
    "## ToDo:\n",
    "- Data Agumentation\n",
    "    - Transformations nebenläufig oder direkt auf das dataset anwenden?\n",
    "- eigenes CNN vom Aufbau entwickeln\n",
    "    - ggf. an den Aufbau von der VL halten\n",
    "    - ggf. Study raussuchen\n",
    "- CNN Speichern können\n",
    "    - CNN nutzen um Testdaten zu analysieren und Ergebnis in CSV schreiben\n",
    "- Accuracy herausfinden\n",
    "- optimale Learning Rate herausfinden\n",
    "    - https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b209c5c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f5182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:07.979975115Z",
     "start_time": "2023-06-01T12:33:06.517127573Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c326ec2",
   "metadata": {},
   "source": [
    "## Load and transform datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39898ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:11.818578210Z",
     "start_time": "2023-06-01T12:33:11.731034193Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "\n",
    "mean = [0.4766, 0.4527, 0.3926]\n",
    "std = [0.2275, 0.2224, 0.2210]\n",
    "\n",
    "meanOriginal = [0.5, 0.5, 0.5]\n",
    "stdOriginal = [0.5, 0.5, 0.5]\n",
    "\n",
    "#transformer for normalization of images\n",
    "# transforms.Compose -> pipeline that in this case firstly converts image to tensor and then normalizes it\n",
    "# - first (0.5, 0.5, 0.5) are the mean for each channel(red, green, blue) and \n",
    "# - second is the standard deviation for each channel\n",
    "# - resize the pictures from 275x183 to 28x28\n",
    "transform =  transforms.Compose(\n",
    "    [transforms.Pad(25, padding_mode='symmetric'),\n",
    "     transforms.RandomHorizontalFlip(), \n",
    "     transforms.ToTensor(),\n",
    "     transforms.Resize((64,64), antialias=False),\n",
    "     transforms.Normalize(mean, std)])\n",
    "\n",
    "##### Testset ohne transformations???\n",
    "transformOriginal =  transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize((16,16), antialias=False),\n",
    "     transforms.Normalize(meanOriginal, stdOriginal)])\n",
    "\n",
    "\n",
    "#batch, to have a certain amount of example pictures\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "#trainset\n",
    "train_dir = '/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/train'\n",
    "#testset\n",
    "test_dir = '/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/test'\n",
    "\n",
    "#load trainset\n",
    "train_set = ImageFolder(train_dir,transform = transform)\n",
    "#load testset\n",
    "test_set = ImageFolder(test_dir,transform = transformOriginal)\n",
    "\n",
    "\n",
    "#calculate the size of the training and validation set \n",
    "train_size = int(0.8 * len(train_set))  # 80% for training\n",
    "val_size = len(train_set) - train_size\n",
    "\n",
    "#Split train_set into training and validation set\n",
    "train_data, val_data = random_split(train_set,[train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#Return batch given transformations of the ImageFolder\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle = True, num_workers = 0)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size, shuffle = True, num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5baa684",
   "metadata": {},
   "source": [
    "### Print random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaffac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:16.821971011Z",
     "start_time": "2023-06-01T12:33:15.800537172Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#Get classes (folder names) of the training set\n",
    "classes = train_set.classes\n",
    "\n",
    "#functions to show an image out of the Training set\n",
    "#print(\"Follwing classes are there : \\n\",train_set.classes)\n",
    "def display_img(img,label):\n",
    "    print(f\"Label : {train_set.classes[label]}\")\n",
    "    img = img / 2 + 0.5 \n",
    "    img = np.clip(img, 0, 1) # Clip values to the valid range [0, 1]\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "#display the first image [0] in the dataset\n",
    "display_img(*train_set[0])    # * to unpack the tuple: (img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a07a3",
   "metadata": {},
   "source": [
    "### Print first 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = train_data[i]\n",
    "    X = X.transpose(0, -1).transpose(0, 1)\n",
    "    X = np.clip(X, 0, 1)  # Clip values to the valid range [0, 1]\n",
    "    y = train_set.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y}', fontsize='small')\n",
    "\n",
    "# Hat das val_data images aus allen Klassen?\n",
    "# -> ja\n",
    "'''\n",
    "arrY = []\n",
    "for i in range(val_size):\n",
    "    X,y = val_data[i]\n",
    "    arrY.append(y)\n",
    "    \n",
    "uArr = np.unique(arrY)\n",
    "uArr\n",
    "'''\n",
    "#for i, batch in enumerate(valloader):\n",
    "#    X, y = batch\n",
    "#    print(i, X.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5062c2",
   "metadata": {},
   "source": [
    "# CNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae62eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:52:41.199420051Z",
     "start_time": "2023-06-01T12:52:41.150465498Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, *layers, lr=0.01, classes=None):\n",
    "        super().__init__() # <- Very important! __init__ from parent class torch.nn \n",
    "        self.lr = lr    # learning rate\n",
    "        self.classes = classes  # classes \n",
    "        ## Build model\n",
    "        self.layers = nn.Sequential(*layers) # Create a sequential model, see below\n",
    "    \n",
    "    # Forward pass through the given layers, needed by torch.nn, which is used by train_step\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "    # Used after training to predict with the generated model\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X).argmax(1)\n",
    "        if self.classes is not None:\n",
    "            y_hat = [self.classes[i] for i in y_hat]\n",
    "        return y_hat\n",
    "    \n",
    "    # Actual forward pass\n",
    "    def train_step(self, X, y):\n",
    "        y_hat = self(X)   # y_hat = prediction, self(X) calls the forward() function above\n",
    "        # y_hat needs to be the shape (batch_size, classes)\n",
    "        # y needs to be the shape of batch_size and contain the class of each sample\n",
    "        # calculates the \"cross_entropy\"-loss, is prediction == actual image\n",
    "        return F.cross_entropy(y_hat, y)\n",
    "        \n",
    "    def validation_step(self, X, y):\n",
    "        # temporarily disable gradient, because you don't need it during validation\n",
    "        with torch.no_grad():\n",
    "            # calculate the loss, just without the gradient-calculation\n",
    "            return self.train_step(X, y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # self.parameters() = returns an iterator over all the learnable parameters of the model\n",
    "        # SGD = optimization alg. for nn\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "        \n",
    "    def fit(self, train_loader, valid_loader, epochs=10):\n",
    "        # Load Optimizer\n",
    "        optimizer = self.configure_optimizers()\n",
    "        \n",
    "        # Amount of epochs, one epoch is when all batches are fed into the model to train at once\n",
    "        for epoch in range(epochs):\n",
    "            print(f'{epoch+1}/{epochs}:')\n",
    "            # Training\n",
    "            self.train() # Set model to training model\n",
    "            # tqm = loading bar, tepoch == array of all batches\n",
    "            with tqdm(train_loader, unit=\"batch\", desc='Train: ', colour='#1f77b4',\\\n",
    "                      file=sys.stdout, ncols=80) as tepoch:\n",
    "                # for each batch in tepoch\n",
    "                correct = 0\n",
    "                for X,y in tepoch:\n",
    "                    optimizer.zero_grad(set_to_none=True) # Sets all gradients to Zero\n",
    "                    loss = self.train_step(X, y) # Execute Forward pass\n",
    "                    loss.backward() # Execute Backwval_dataard pass\n",
    "                    optimizer.step() # Update weights\n",
    "                    tepoch.set_postfix(loss=f'{loss.item():1.4f}') # Update progress bar\n",
    "                    \n",
    "            # Validation\n",
    "            self.eval() # Set model to validation mode\n",
    "            with tqdm(train_loader, unit=\"batch\", desc='Valid: ', colour='#ff7f0e',\\\n",
    "                      file=sys.stdout, ncols=80) as tepoch:\n",
    "               \n",
    "                for X,y in tepoch:\n",
    "                    loss_valid = self.validation_step(X, y) # Execute Forward pass without gradients\n",
    "                    tepoch.set_postfix(loss=f'{loss_valid.item():1.4f}') # Update progress bar\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65311f0",
   "metadata": {},
   "source": [
    "## Make .csv with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "class MakeCSVPredictions:\n",
    "    def __init__(self, test_set, model, csv_file):\n",
    "        self.test_set = test_set\n",
    "        self.model = model\n",
    "        self.csv_file = csv_file\n",
    "    \n",
    "    def writeToCSV(self):\n",
    "        filenames = []\n",
    "        for image_path, _ in self.test_set.samples:\n",
    "            filename = os.path.basename(image_path)\n",
    "            filenames.append(filename)\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(len(self.test_set)): \n",
    "            X,y = self.test_set[i]\n",
    "            y_hat = self.model.predict(X.unsqueeze(0))[0]\n",
    "            predictions.append(y_hat)\n",
    "\n",
    "\n",
    "        data = np.column_stack((filenames, predictions))\n",
    "        with open(self.csv_file, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the data to the CSV file\n",
    "            writer.writerows(data)\n",
    "\n",
    "        print(\"CSV file written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06442ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to run desired model first, to run this block\n",
    "csv_file = '/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/prediction.csv'\n",
    "MakeCSVPredictions(test_set, model1, csv_file).writeToCSV()\n",
    "\n",
    "print(AnalyseModel(val_data, model1, train_set.classes).getAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5b59f",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb014459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyseModel:\n",
    "    def __init__(self, val_set, model, classes):\n",
    "        self.val_set = val_set\n",
    "        self.model = model\n",
    "        self.classes = classes\n",
    "    \n",
    "    def getAccuracy(self):\n",
    "        predictions = []\n",
    "        ys = []\n",
    "        for i in range(len(self.val_set)): \n",
    "            X,y = self.val_set[i]\n",
    "            y_hat = self.model.predict(X.unsqueeze(0))[0]\n",
    "            y = self.classes[y]\n",
    "            predictions.append(y_hat)\n",
    "            ys.append(y)\n",
    "\n",
    "        count = 0\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == ys[i]:\n",
    "                count += 1\n",
    "\n",
    "        accuracy = count / len(predictions)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0539ba",
   "metadata": {},
   "source": [
    "# Model 1 - First Simple Layered CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7118e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:53:24.166976892Z",
     "start_time": "2023-06-01T12:52:44.094270534Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = MyCNNModel(\n",
    "    # different functions from torch.nn to modify the model \n",
    "    nn.Conv2d(3, 6, (5,5), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(6, 16, (5,5), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20), # 20 because 20 classes\n",
    "    classes=train_set.classes,\n",
    ").fit(trainloader, valloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95298a53",
   "metadata": {},
   "source": [
    "## Model 1 - Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), '/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/model1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125a247",
   "metadata": {},
   "source": [
    "## Model 1 - Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ec50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MyCNNModel(\n",
    "    # different functions from torch.nn to modify the model \n",
    "    nn.Conv2d(3, 6, (5,5), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(6, 16, (5,5), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20), # 20 because 20 classes\n",
    "    classes=train_set.classes,\n",
    ")\n",
    "model1.load_state_dict(torch.load('/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/model1.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a72f74",
   "metadata": {},
   "source": [
    "## Model 1 - Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(20, 20))\n",
    "for i in range(10):\n",
    "    X,y = train_set[i]\n",
    "    y_hat = model1.predict(X.unsqueeze(0))[0]\n",
    "    X = X.transpose(0, -1).transpose(0, 1)\n",
    "    X = np.clip(X, 0, 1)  # Clip values to the valid range [0, 1]\n",
    "    y = train_set.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y} - {y_hat}', fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203f6c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "759dcbce",
   "metadata": {},
   "source": [
    "# Model 2 - More ReLUs and Convolutions CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74eaa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MyCNNModel(\n",
    "    nn.Conv2d(3, 8, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 8, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(8, 16, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20),\n",
    "    classes=testset.classes,\n",
    ").fit(trainloader, valloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd719489",
   "metadata": {},
   "source": [
    "## Model 2 - Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = testset[i]\n",
    "    y_hat = model2.predict(X.unsqueeze(0))[0]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, testset.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y} - {y_hat}', fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70eb4b",
   "metadata": {},
   "source": [
    "# Model 3 - ResNet full-preactivation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the full-preactivation ResNet Layer\n",
    "class MyResLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, **kwargs):\n",
    "        super().__init__()\n",
    "        if in_channels == out_channels:\n",
    "            self.proj_out = nn.Identity()\n",
    "        else:\n",
    "            self.proj_out = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "            \n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels), #BN\n",
    "            nn.ReLU(), #ReLU\n",
    "            nn.Conv2d(in_channels, out_channels, kernel, **kwargs), #weight\n",
    "            nn.BatchNorm2d(out_channels), #BN\n",
    "            nn.ReLU(), #ReLU\n",
    "            nn.Conv2d(out_channels, out_channels, kernel, **kwargs), #weight\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_res = self.res_block(x)\n",
    "        x = self.proj_out(x)\n",
    "        return  F.relu(x + x_res) #x + x_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2099b",
   "metadata": {},
   "source": [
    "## Model 3 - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6364f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MyCNNModel(\n",
    "    MyResLayer(3, 8, (3,3), padding='same'),\n",
    "    MyResLayer(8, 8, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(8, 16, (3,3), padding='same'),\n",
    "    MyResLayer(16, 16, (3,3), padding='same'),\n",
    "    MyResLayer(16, 16, (3,3), padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20),\n",
    "    classes=train_set.classes,\n",
    ").fit(trainloader, valloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fb2c4",
   "metadata": {},
   "source": [
    "## Model 3 - Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903465d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120ae9ef",
   "metadata": {},
   "source": [
    "## Model 3 - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be074a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AnalyseModel(val_data, model3, train_set.classes).getAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1a944",
   "metadata": {},
   "source": [
    "# Model 4 - Inception Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Inception module with dimension reductions\n",
    "class MyInceptionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels//4\n",
    "        self.conv1x = nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "        # change input dimension\n",
    "        self.conv3x_afterconv1x = nn.Conv2d(out_channels, out_channels, (3,3), **kwargs)\n",
    "        self.conv5x_afterconv1x = nn.Conv2d(out_channels, out_channels, (5,5), **kwargs)\n",
    "        \n",
    "        self.poolx = nn.Sequential(\n",
    "            nn.MaxPool2d((3,3), stride=(1,1), padding=1),\n",
    "            nn.Conv2d(in_channels, out_channels, (1,1), **kwargs)\n",
    "        )\n",
    "        # change input dimension\n",
    "        self.conv1x_afterpoolx = nn.Conv2d(out_channels, out_channels, (1,1), **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_1 = self.conv1x(x)\n",
    "        \n",
    "        # feed the 1x1 conv into the 3x3 conv\n",
    "        x_1x_3 = self.conv3x_afterconv1x(self.conv1x(x))\n",
    "        # feed the 1x1 conv into the 5x5 conv\n",
    "        x_1x_5 = self.conv3x_afterconv1x(self.conv1x(x))\n",
    "        # feed the 3x3 max pooling into the modified 1x1 conv\n",
    "        x_px_1 = self.conv1x_afterpoolx(self.poolx(x))\n",
    "        \n",
    "        # concatenate the outputs of the convolutional operations\n",
    "        x = torch.cat([x_1, x_1x_3, x_1x_5, x_px_1], dim=1)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcdb823",
   "metadata": {},
   "source": [
    "## Model 4 - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ecc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MyCNNModel(\n",
    "    MyInceptionLayer(3, 8, padding='same'),\n",
    "    MyInceptionLayer(8, 8, padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyInceptionLayer(8, 16, padding='same'),\n",
    "    MyInceptionLayer(16, 16, padding='same'),\n",
    "    MyInceptionLayer(16, 16, padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20),\n",
    "    classes=train_set.classes,\n",
    ").fit(trainloader, valloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1ad67",
   "metadata": {},
   "source": [
    "## Model 4 - Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19da318c",
   "metadata": {},
   "source": [
    "## Model 4 - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AnalyseModel(val_data, model4, train_set.classes).getAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128398f",
   "metadata": {},
   "source": [
    "# Model 5 - Mixing ResNet with Inception CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c35a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = MyCNNModel(\n",
    "    MyResLayer(3, 8, (3,3), padding='same'),\n",
    "    MyResLayer(8, 8, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(8, 16, (3,3), padding='same'),\n",
    "    MyResLayer(16, 16, (3,3), padding='same'),\n",
    "    MyInceptionLayer(16, 16, padding='same'),\n",
    "    MyInceptionLayer(16, 16, padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20),\n",
    "    classes=train_set.classes,\n",
    ").fit(trainloader, valloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32363b5",
   "metadata": {},
   "source": [
    "## Model 5 - Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = train_set[i]\n",
    "    y_hat = model4.predict(X.unsqueeze(0))[0]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, train_set.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y} - {y_hat}', fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2e472",
   "metadata": {},
   "source": [
    "## Model 5 - Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model5.state_dict(), '/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/model5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef8bc60",
   "metadata": {},
   "source": [
    "## Model 5 - Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679bec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = MyCNNModel(\n",
    "    MyResLayer(3, 8, (3,3), padding='same'),\n",
    "    MyResLayer(8, 8, (3,3), padding='same'),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    MyResLayer(8, 16, (3,3), padding='same'),\n",
    "    MyResLayer(16, 16, (3,3), padding='same'),\n",
    "    MyInceptionLayer(16, 16, padding='same'),\n",
    "    MyInceptionLayer(16, 16, padding='same'),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20),\n",
    "    classes=train_set.classes,\n",
    ")\n",
    "model5.load_state_dict(torch.load('/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/model5.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b93757",
   "metadata": {},
   "source": [
    "## Model 5 - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AnalyseModel(val_data, model5, train_set.classes).getAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa8b63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8c657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53d45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

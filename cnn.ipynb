{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b209c5c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345f5182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T20:32:23.097928274Z",
     "start_time": "2023-05-29T20:32:21.648111616Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c326ec2",
   "metadata": {},
   "source": [
    "### load and normalize dog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39898ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T20:32:26.574283680Z",
     "start_time": "2023-05-29T20:32:26.475531211Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "\n",
    "#transformer for normalization of images\n",
    "# transforms.Compose -> pipeline that in this case firstly converts image to tensor and then normalizes it\n",
    "# - first (0.5, 0.5, 0.5) are the mean for each channel(red, green, blue) and \n",
    "# - second is the standard deviation for each channel\n",
    "# - resize the pictures from 275x183 to 28x28\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize((28,28), antialias=False),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#batch, to have a certain amount of example pictures\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "#trainset\n",
    "train_dir = '/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/train'\n",
    "#testset\n",
    "test_dir = '/home/ck/Documents/Deep_Learning_Python/convolutional_neural_network/test'\n",
    "\n",
    "#load trainset\n",
    "train_set = ImageFolder(train_dir,transform = transform)\n",
    "\n",
    "#load testset\n",
    "test_set = ImageFolder(test_dir,transform = transform)\n",
    "\n",
    "\n",
    "\n",
    "#calculate the size of the training and validation set \n",
    "train_size = int(0.8 * len(train_set))  # 80% for training\n",
    "val_size = len(train_set) - train_size\n",
    "\n",
    "#Split train_set into training and validation set\n",
    "train_data, val_data = random_split(train_set,[train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#?????\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle = True, num_workers = 0)\n",
    "valloader = DataLoader(val_data, batch_size=batch_size, shuffle = True, num_workers = 0)\n",
    "\n",
    "\n",
    "testloader = DataLoader(test_set, batch_size=batch_size, shuffle = False, num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de8833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5baa684",
   "metadata": {},
   "source": [
    "### Print random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abaffac",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-05-29T20:32:29.132420430Z",
     "start_time": "2023-05-29T20:32:28.516813712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : African_hunting_dog\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Get classes (folder names) of the training set\n",
    "classes = train_set.classes\n",
    "\n",
    "#functions to show an image out of the Training set\n",
    "#print(\"Follwing classes are there : \\n\",train_set.classes)\n",
    "def display_img(img,label):\n",
    "    print(f\"Label : {train_set.classes[label]}\")\n",
    "    img = img / 2 + 0.5 # unnormalize picture\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "#display the first image [0] in the dataset\n",
    "display_img(*train_set[0])    # * to unpack the tuple: (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = train_data[i]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, train_set.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y}', fontsize='small')\n",
    "\n",
    "# Hat das val_data images aus allen Klassen?\n",
    "# -> ja\n",
    "'''\n",
    "arrY = []\n",
    "for i in range(val_size):\n",
    "    X,y = val_data[i]\n",
    "    arrY.append(y)\n",
    "    \n",
    "uArr = np.unique(arrY)\n",
    "uArr\n",
    "'''\n",
    "for i, batch in enumerate(valloader):\n",
    "    X, y = batch\n",
    "    print(i, X.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae62eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T20:32:37.480994747Z",
     "start_time": "2023-05-29T20:32:37.472699971Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, *layers, lr=0.01, classes=None):\n",
    "        super().__init__() # <- Very important! __init__ from parent class torch.nn \n",
    "        self.lr = lr    # learning rate\n",
    "        self.classes = classes  # classes \n",
    "        ## Build model\n",
    "        self.layers = nn.Sequential(*layers) # Create a sequential model, see below\n",
    "    \n",
    "    # Forward pass through the given layers, needed by torch.nn, which is used by train_step\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "    # Used after training to predict with the generated model\n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(X).argmax(1)\n",
    "        if self.classes is not None:\n",
    "            y_hat = [self.classes[i] for i in y_hat]\n",
    "        return y_hat\n",
    "    \n",
    "    # Actual forward pass\n",
    "    def train_step(self, X, y):\n",
    "        y_hat = self(X)   # y_hat = prediction, self(X) calls the forward() function above\n",
    "        # y_hat needs to be the shape (batch_size, classes)\n",
    "        # y needs to be the shape of batch_size and contain the class of each sample\n",
    "        # calculates the \"cross_entropy\"-loss, is y_hat == y\n",
    "        return F.cross_entropy(y_hat, y) \n",
    "        \n",
    "    def validation_step(self, X, y):\n",
    "        # temporarily disable gradient, because you don't need it during validation\n",
    "        with torch.no_grad():\n",
    "            # calculate the loss, just without the gradient-calculation\n",
    "            return self.train_step(X, y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # self.parameters() = returns an iterator over all the learnable parameters of the model\n",
    "        # SGD = optimization alg. for nn\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "        \n",
    "    def fit(self, train_loader, valid_loader, epochs=10):\n",
    "        # Load Optimizer\n",
    "        optimizer = self.configure_optimizers()\n",
    "        \n",
    "        # Amount of epochs, one epoch is when all batches are fed into the model to train at once\n",
    "        for epoch in range(epochs):\n",
    "            print(f'{epoch+1}/{epochs}:')\n",
    "            # Training\n",
    "            self.train() # Set model to training mode\n",
    "            # tqm = loading bar, tepoch == array of all batches\n",
    "            with tqdm(train_loader, unit=\"batch\", desc='Train: ', colour='#1f77b4',\\\n",
    "                      file=sys.stdout, ncols=80) as tepoch:\n",
    "                # for each batch in tepoch\n",
    "                for X,y in tepoch:\n",
    "                    optimizer.zero_grad(set_to_none=True) # Sets all gradients to Zero\n",
    "                    loss = self.train_step(X, y) # Execute Forward pass\n",
    "                    loss.backward() # Execute Backwval_dataard pass\n",
    "                    optimizer.step() # Update weights\n",
    "                    tepoch.set_postfix(loss=f'{loss.item():1.4f}') # Update progress bar\n",
    "                    \n",
    "            # Validation\n",
    "            self.eval() # Set model to validation mode\n",
    "            with tqdm(train_loader, unit=\"batch\", desc='Valid: ', colour='#ff7f0e',\\\n",
    "                      file=sys.stdout, ncols=80) as tepoch:\n",
    "                for X,y in tepoch:\n",
    "                    loss_valid = self.validation_step(X, y) # Execute Forward pass without gradients\n",
    "                    tepoch.set_postfix(loss=f'{loss_valid.item():1.4f}') # Update progress bar\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e7118e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T20:33:07.061235300Z",
     "start_time": "2023-05-29T20:32:41.811140391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10:\n",
      "Train:  23%|\u001B[38;2;31;119;180m███▉             \u001B[0m| 372/1586 [00:23<01:17, 15.71batch/s, loss=2.3752]\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model1 \u001B[38;5;241m=\u001B[39m \u001B[43mMyCNNModel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# different functions from torch.nn to modify the model \u001B[39;49;00m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mConv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msame\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mReLU\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMaxPool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mConv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msame\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mReLU\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdaptiveMaxPool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFlatten\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# 20 because 20 classes\u001B[39;49;00m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[4], line 58\u001B[0m, in \u001B[0;36mMyCNNModel.fit\u001B[0;34m(self, train_loader, valid_loader, epochs)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# tqm = loading bar, tepoch == array of all batches\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(train_loader, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain: \u001B[39m\u001B[38;5;124m'\u001B[39m, colour\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m#1f77b4\u001B[39m\u001B[38;5;124m'\u001B[39m,\\\n\u001B[1;32m     56\u001B[0m           file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mstdout, ncols\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m80\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m tepoch:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# for each batch in tepoch\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m X,y \u001B[38;5;129;01min\u001B[39;00m tepoch:\n\u001B[1;32m     59\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad(set_to_none\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;66;03m# Sets all gradients to Zero\u001B[39;00m\n\u001B[1;32m     60\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_step(X, y) \u001B[38;5;66;03m# Execute Forward pass\u001B[39;00m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/tqdm/std.py:1178\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1175\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1178\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1179\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1180\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1181\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    228\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[0;32m--> 229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001B[0m, in \u001B[0;36mdefault_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001B[0m, in \u001B[0;36mpil_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpil_loader\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image\u001B[38;5;241m.\u001B[39mImage:\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m--> 247\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/PIL/Image.py:3277\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3274\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m   3275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 3277\u001B[0m im \u001B[38;5;241m=\u001B[39m \u001B[43m_open_core\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformats\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m formats \u001B[38;5;129;01mis\u001B[39;00m ID:\n\u001B[1;32m   3280\u001B[0m     checked_formats \u001B[38;5;241m=\u001B[39m formats\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/PIL/Image.py:3263\u001B[0m, in \u001B[0;36mopen.<locals>._open_core\u001B[0;34m(fp, filename, prefix, formats)\u001B[0m\n\u001B[1;32m   3261\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m result:\n\u001B[1;32m   3262\u001B[0m     fp\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m-> 3263\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[43mfactory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3264\u001B[0m     _decompression_bomb_check(im\u001B[38;5;241m.\u001B[39msize)\n\u001B[1;32m   3265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m im\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:822\u001B[0m, in \u001B[0;36mjpeg_factory\u001B[0;34m(fp, filename)\u001B[0m\n\u001B[1;32m    821\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mjpeg_factory\u001B[39m(fp\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 822\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[43mJpegImageFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    823\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    824\u001B[0m         mpheader \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39m_getmp()\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/PIL/ImageFile.py:103\u001B[0m, in \u001B[0;36mImageFile.__init__\u001B[0;34m(self, fp, filename)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoderconfig \u001B[38;5;241m=\u001B[39m ()\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecodermaxblock \u001B[38;5;241m=\u001B[39m MAXBLOCK\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mis_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;66;03m# filename\u001B[39;00m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilename \u001B[38;5;241m=\u001B[39m fp\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/PIL/_util.py:5\u001B[0m, in \u001B[0;36mis_path\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_path\u001B[39m(f):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f, (\u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m, Path))\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_directory\u001B[39m(f):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model1 = MyCNNModel(\n",
    "    # different functions from torch.nn to modify the model \n",
    "    nn.Conv2d(3, 6, (5,5), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(6, 16, (5,5), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 20), # 20 because 20 classes\n",
    "    classes=train_set.classes,\n",
    ").fit(trainloader, valloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a72f74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = testset[i]\n",
    "    y_hat = model1.predict(X.unsqueeze(0))[0]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, testset.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y} - {y_hat}', fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dcbce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74eaa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MyCNNModel(\n",
    "    nn.Conv2d(3, 8, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(8, 8, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(8, 16, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, (3,3), padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(1),\n",
    "    nn.Linear(16, 10),\n",
    "    classes=testset.classes,\n",
    ").fit(trainloader, testloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd719489",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,10, figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    X,y = testset[i]\n",
    "    y_hat = model2.predict(X.unsqueeze(0))[0]\n",
    "    X,y = X.transpose(0,-1).transpose(0,1) * 0.5 + 0.5, testset.classes[y]\n",
    "    axarr[i].imshow(X)\n",
    "    axarr[i].axis('off')\n",
    "    axarr[i].set_title(f'{y} - {y_hat}', fontsize='small')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
